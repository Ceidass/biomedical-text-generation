{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2474343",
   "metadata": {},
   "source": [
    "# 03 â€“ Data Scanning & Token Analysis\n",
    "\n",
    "In this notebook, we perform a **preliminary scan and token-based analysis** of the cleaned dataset in order to:\n",
    "\n",
    "- Estimate the token length of abstracts and titles,\n",
    "- Determine appropriate values for `max_input_length` / `max_output_length`,\n",
    "-Provide visual and statistical insights before constructing the final training datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a7f9fc",
   "metadata": {},
   "source": [
    "# Entity Analysis (without UMLS Linking)\n",
    "\n",
    "This notebook performs entity frequency analysis on biomedical abstracts using the `en_core_sci_lg` model from SciSpaCy. It extracts biomedical terms and identifies the most common multi-word entities.\n",
    "\n",
    "## What This Does\n",
    "- Loads pre-processed abstracts (with extracted entities)\n",
    "- Counts and filters entities based on word count (e.g., only 2+ word phrases)\n",
    "- Displays and saves the most frequent multi-word biomedical terms\n",
    "\n",
    "## What This Does NOT Do\n",
    "This version does **not** use UMLS entity linking due to technical limitations (`nmslib` build issues on Colab). As a result:\n",
    "- **Synonyms are not merged** (e.g., `NSCLC` â‰  `non-small cell lung cancer`)\n",
    "- **Entities are not normalized** to UMLS concept IDs\n",
    "- **No abbreviation resolution** or semantic types are included\n",
    "\n",
    "## Future Extension\n",
    "If UMLS linking is enabled in the future:\n",
    "- Terms will be linked to UMLS concepts (e.g., `C0007131`)\n",
    "- Synonyms and abbreviations will be merged into canonical forms\n",
    "- Enriched analysis with metadata (semantic types, vocabulary links, etc.)\n",
    "\n",
    "Despite this limitation, the current entity recognition pipeline still offers strong results for exploratory analysis and keyword extraction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568da13b",
   "metadata": {},
   "source": [
    "## ðŸ’¡ Example: With vs. Without UMLS Linking\n",
    "\n",
    "### Without UMLS Linking (Current Behavior)\n",
    "Entities are extracted as plain text, without normalization:\n",
    "\n",
    "- `NSCLC`\n",
    "- `non-small cell lung cancer`\n",
    "- `Non-Small Cell Lung Carcinoma`\n",
    "- `lung adenocarcinoma`\n",
    "\n",
    "Each of these will be treated as **distinct entities** â€” even though they refer to the same medical concept.\n",
    "\n",
    "---\n",
    "\n",
    "### With UMLS Linking (Desired Behavior)\n",
    "Entities are mapped to UMLS concepts, allowing for:\n",
    "\n",
    "- **Synonym merging**\n",
    "- **Abbreviation resolution**\n",
    "- **Concept-level normalization**\n",
    "\n",
    "For example:\n",
    "\n",
    "| Text Span                           | Linked UMLS Concept | Concept Name                  |\n",
    "|------------------------------------|----------------------|-------------------------------|\n",
    "| `NSCLC`                            | C0007131             | Non-Small Cell Lung Carcinoma |\n",
    "| `non-small cell lung cancer`       | C0007131             | Non-Small Cell Lung Carcinoma |\n",
    "| `lung adenocarcinoma`              | C0007131             | Non-Small Cell Lung Carcinoma |\n",
    "\n",
    "This makes downstream analysis, clustering, and retrieval much more robust and semantically meaningful.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086326a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only if you are using Google Colab and want to retreive the data from your Google Drive.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aaa7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load enriched biomedical abstracts (with extracted entities)\n",
    "with open(\"/content/drive/MyDrive/biomedical_text_generation/data/enriched/abstracts_with_entities.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "df = pd.DataFrame(data)\n",
    "df[[\"title\", \"entities\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed30529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten all entities from all abstracts into a single list\n",
    "all_entities = [entity for entry in df[\"entities\"] for entity in entry]\n",
    "print(f\"Total entities collected: {len(all_entities)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e9eabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only entities with 2 or more words\n",
    "multi_word_entities = [ent for ent in all_entities if len(ent.split()) >= 2]\n",
    "print(f\"Multi-word entities (2+ words): {len(multi_word_entities)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e754cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count frequency of multi-word entities\n",
    "entity_counter = Counter(multi_word_entities)\n",
    "\n",
    "# Get top 300 (Computed to have over 100 appearances)\n",
    "top_entities = entity_counter.most_common(350)\n",
    "\n",
    "# Preview top 20\n",
    "print(\"Top 20 multi-word entities:\")\n",
    "for entity, count in top_entities[:20]:\n",
    "    print(f\"{entity}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31421a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define output path\n",
    "output_path = \"/content/drive/MyDrive/biomedical_text_generation/data/processed/top_multiword_entities.json\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "# Save to JSON file\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(top_entities, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n Saved top multi-word entities to: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
