{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2474343",
   "metadata": {},
   "source": [
    "# 03 â€“ Data Scanning & Token Analysis\n",
    "\n",
    "In this notebook, we perform a **preliminary scan and token-based analysis** of the cleaned dataset in order to:\n",
    "\n",
    "- Estimate the token length of abstracts and titles,\n",
    "- Determine appropriate values for `max_input_length` / `max_output_length`,\n",
    "-Provide visual and statistical insights before constructing the final training datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1599cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only if you are using Google Colab and want to retreive the data from your Google Drive.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8607fc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports\n",
    "import json\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d213c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading and first check\n",
    "\n",
    "# Loading the cleaned dataset\n",
    "with open(\"/content/drive/MyDrive/biomedical_text_generation/data/cleaned/all_abstracts_cleaned.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert to DataFrame for preview\n",
    "df = pd.DataFrame(data)\n",
    "df.sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d43805",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Select tokenizer - for example the T5 base\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "# Calaculating title lengths and abstracts in tokens\n",
    "df[\"title_tokens\"] = df[\"title\"].apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "df[\"abstract_tokens\"] = df[\"abstract\"].apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "\n",
    "# Statistics\n",
    "print(\"Title Token\")\n",
    "print(df[\"title_tokens\"].describe())\n",
    "print(\"\\nAbstract Token\")\n",
    "print(df[\"abstract_tokens\"].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd04cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "\n",
    "plt.hist(df[\"abstract_tokens\"], bins=40, color=\"skyblue\", edgecolor=\"black\")\n",
    "plt.title(\"Token Length Distribution of Abstracts (T5 tokenizer)\")\n",
    "plt.xlabel(\"Number of tokens\")\n",
    "plt.ylabel(\"Number of examples\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82991b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating directory if does not exist\n",
    "os.makedirs(\"/content/drive/MyDrive/biomedical_text_generation/data/processed\", exist_ok=True)\n",
    "\n",
    "# Saving a copy of enriched dataset (with token counts)\n",
    "df.to_json(\"/content/drive/MyDrive/biomedical_text_generation/data/processed/abstracts_with_tokens.json\", orient=\"records\", force_ascii=False, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
