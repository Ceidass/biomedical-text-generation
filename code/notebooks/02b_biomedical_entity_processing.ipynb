{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7a65222",
   "metadata": {},
   "source": [
    "# ðŸ“˜ 02b â€“ Biomedical Entity Processing\n",
    "\n",
    "In this notebook, we extract and normalize biomedical entities from cleaned abstracts using **SciSpacy** and **UMLS linking**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83663383",
   "metadata": {},
   "source": [
    "## Biomedical Entity Processing (without UMLS Linking)\n",
    "\n",
    "In this notebook, we extract biomedical entities from research abstracts using the `en_core_sci_lg` model from [SciSpaCy](https://allenai.github.io/scispacy/).\n",
    "\n",
    "Originally, the plan was to also integrate the **UMLS Entity Linker** (`UmlsEntityLinker`), which would allow us to:\n",
    "\n",
    "- Normalize entities to unified UMLS concept identifiers\n",
    "- Resolve abbreviations (e.g., `HTN` â†’ `Hypertension`)\n",
    "- Merge synonyms under the same concept (e.g., `non-small cell lung cancer`, `NSCLC`)\n",
    "- Enable concept-level analysis and retrieval\n",
    "\n",
    "However, due to persistent issues with dependencies â€” mainly the failure to install `nmslib` in the Colab environment â€” we had to skip UMLS linking for now.\n",
    "\n",
    "### What we achieved:\n",
    "- Extracted entities directly from the text using a pretrained biomedical model.\n",
    "- Cleaned and enriched the dataset with these entities.\n",
    "- Saved the enriched abstracts for further analysis.\n",
    "\n",
    "The extracted entities are **surface-level mentions** from the text. While less structured than UMLS concepts, they still provide a strong basis for downstream analysis â€” such as frequency analysis, relevance filtering, and potential future retrieval tasks.\n",
    "\n",
    "We keep the pipeline modular so that **UMLS linking can be added later** with minimal changes if we manage to resolve the installation issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2e0a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only if you are using Google Colab\n",
    "\n",
    "# Downgrade NumPy to a version that works with spaCy 3.4.4 and scispaCy 0.5.1\n",
    "!pip install numpy==1.23.5 --force-reinstall -q\n",
    "\n",
    "# Install spaCy 3.4.4\n",
    "!pip install spacy==3.4.4 -q\n",
    "\n",
    "# Install scispaCy 0.5.1 (biomedical NLP extensions for spaCy)\n",
    "!pip install scispacy==0.5.1 -q\n",
    "\n",
    "# Download and install the en_core_sci_lg biomedical model\n",
    "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_lg-0.5.1.tar.gz -q\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1349be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only if you are using Google Colab and want to retreive the data from your Google Drive.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa042fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_sci_lg\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Load the pretrained SciSpaCy biomedical model\n",
    "nlp = en_core_sci_lg.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6be949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned abstracts (assumed already preprocessed and cleaned)\n",
    "with open(\"/content/drive/MyDrive/biomedical_text_generation/data/cleaned/all_abstracts_cleaned.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Output number of abstracts loaded\n",
    "print(f\"Total abstracts loaded: {len(data)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef3d68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list to store processed abstracts with entities\n",
    "enriched_data = []\n",
    "\n",
    "# Process a sample (first 200 entries) for faster iteration\n",
    "for entry in tqdm(data):\n",
    "    abstract = entry[\"abstract\"]\n",
    "\n",
    "    # Apply the biomedical NLP pipeline on the abstract\n",
    "    doc = nlp(abstract)\n",
    "\n",
    "    # Extract unique named entities with length > 2 (to skip generic short tokens)\n",
    "    entities = list(set(ent.text for ent in doc.ents if len(ent.text) > 2))\n",
    "\n",
    "    # Append the extracted entities to the original entry\n",
    "    entry[\"entities\"] = entities\n",
    "\n",
    "    # Add to final enriched dataset\n",
    "    enriched_data.append(entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b28b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(\"/content/drive/MyDrive/biomedical_text_generation/data/enriched\", exist_ok=True)\n",
    "\n",
    "# Save the enriched data (with extracted biomedical entities) to a JSON file\n",
    "with open(\"/content/drive/MyDrive/biomedical_text_generation/data/enriched/abstracts_with_entities.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(enriched_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Print confirmation message\n",
    "print(\"Enriched dataset saved with biomedical entities.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ecad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame to preview titles and extracted entities\n",
    "df = pd.DataFrame(enriched_data)\n",
    "df[[\"title\", \"entities\"]].head(10)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
