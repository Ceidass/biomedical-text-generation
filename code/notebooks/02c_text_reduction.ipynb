{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5adcf9d",
   "metadata": {},
   "source": [
    "# Biomedical Abstract Sentence Extraction using YAKE and Pre-Extracted Entities\n",
    "\n",
    "This notebook processes biomedical abstracts that already contain **pre-extracted entities** (e.g., using named entity recognition tools). The goal is to create a concise, informative **summary** of each abstract by selecting only the **most relevant sentences**, using a combination of:\n",
    "\n",
    "- **Keyword extraction (YAKE)**\n",
    "- **Pre-extracted biomedical entities**\n",
    "- **Keyword- and entity-based sentence filtering**\n",
    "\n",
    "## Workflow Summary\n",
    "\n",
    "1. **Input**\n",
    "   - A JSON file containing biomedical abstracts.\n",
    "   - Each abstract includes:\n",
    "     - `pmid`: PubMed ID\n",
    "     - `title`: Title of the paper\n",
    "     - `abstract`: Full abstract text\n",
    "     - `entities`: A list of biomedical terms pre-extracted using tools like SciSpacy.\n",
    "\n",
    "2. **Keyword Extraction (YAKE)**\n",
    "   - YAKE extracts potential keywords and phrases from the abstract.\n",
    "   - Keywords are scored for significance; we retain them without filtering by score initially.\n",
    "\n",
    "3. **Keyword Filtering**\n",
    "   - From the YAKE output, we keep only:\n",
    "     - Exact matches to the entities.\n",
    "     - Phrases that **contain** any of the entities (e.g., \"inflammatory cytokines\" if \"cytokines\" is an entity).\n",
    "   - This helps avoid keyword bloat and ensures relevance.\n",
    "\n",
    "4. **Sentence Matching**\n",
    "   - Sentences in the abstract are tokenized.\n",
    "   - A sentence is retained only if it contains **at least one filtered keyword or entity**.\n",
    "   - This filters the abstract down to its most important parts.\n",
    "\n",
    "5. **Deduplication**\n",
    "   - Repeated keywords and phrases are removed.\n",
    "   - Duplicate sentences are also filtered out, preserving clarity and brevity.\n",
    "\n",
    "6. **Final Output**\n",
    "   Each entry in the output JSON includes:\n",
    "   - `pmid`\n",
    "   - `title`\n",
    "   - `abstract`\n",
    "   - `combined_keywords`: Filtered set of relevant keywords (entities + YAKE)\n",
    "   - `matched_text`: Concise summary built from the most relevant, unique sentences\n",
    "\n",
    "> This approach creates targeted, information-dense summaries of biomedical abstracts, ideal for downstream tasks like indexing, classification, or text generation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d3c6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install YAKE for keyword extraction\n",
    "!pip install git+https://github.com/LIAAD/yake.git -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c19725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dc1ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update with your actual file path\n",
    "input_path = '../../data/enriched/abstracts_with_entities.json'\n",
    "\n",
    "#\n",
    "output_path = '../../data/enriched/abstracts_to_text.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732a7497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake\n",
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "# Download the 'punkt_tab' resource\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456c83a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize YAKE extractor\n",
    "yake_kw_extractor = yake.KeywordExtractor(lan=\"en\", n=10, top=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f75f602",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_path, \"r\") as f:\n",
    "    data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aaff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional. For testing purposes\n",
    "data = data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da344da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def phrase_contains_any(word_set, phrase):\n",
    "    return any(word in phrase.lower().split() for word in word_set)\n",
    "\n",
    "def deduplicate_phrases(phrases):\n",
    "    phrases_sorted = sorted(phrases, key=lambda x: -len(x))\n",
    "    result = []\n",
    "    seen = set()\n",
    "    for phrase in phrases_sorted:\n",
    "        if not any(phrase.lower() in p.lower() and phrase.lower() != p.lower() for p in result):\n",
    "            result.append(phrase)\n",
    "            seen.add(phrase.lower())\n",
    "    return result\n",
    "\n",
    "for entry in tqdm(data):\n",
    "    abstract = entry.get(\"abstract\", \"\")\n",
    "    entities = entry.get(\"entities\", [])\n",
    "\n",
    "    # 1. Extract YAKE keywords\n",
    "    yake_keywords = [kw for kw, score in yake_kw_extractor.extract_keywords(abstract)]\n",
    "\n",
    "    # 2. Save all_entities: union of raw YAKE and entities, deduplicated\n",
    "    all_entities = list(set(map(str.lower, entities + yake_keywords)))\n",
    "    entry[\"all_entities\"] = all_entities\n",
    "\n",
    "    # 3. Filter YAKE keywords that overlap with any entity\n",
    "    entity_words = set(e.lower() for e in entities)\n",
    "    filtered_yake_keywords = [kw for kw in yake_keywords if phrase_contains_any(entity_words, kw)]\n",
    "\n",
    "    # 4. Deduplicate overlapping phrases\n",
    "    deduped_keywords = deduplicate_phrases(filtered_yake_keywords)\n",
    "    entry[\"combined_keywords\"] = deduped_keywords\n",
    "\n",
    "    # 5. Match sentences from abstract\n",
    "    sentences = sent_tokenize(abstract)\n",
    "    matched_sentences = [\n",
    "        sent.strip() for sent in sentences\n",
    "        if any(kw.lower() in sent.lower() for kw in deduped_keywords)\n",
    "    ]\n",
    "\n",
    "    # 6. Deduplicate matched sentences\n",
    "    seen = set()\n",
    "    unique_matched_sentences = []\n",
    "    for sent in matched_sentences:\n",
    "        if sent not in seen:\n",
    "            unique_matched_sentences.append(sent)\n",
    "            seen.add(sent)\n",
    "\n",
    "    # 7. Compose matched text\n",
    "    entry[\"matched_text\"] = \" \".join(\n",
    "        sent if sent.endswith(\".\") else sent + \".\" for sent in unique_matched_sentences\n",
    "    )\n",
    "\n",
    "    # 8. Optional cleanup: remove original entities field\n",
    "    entry.pop(\"entities\", None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13522da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(data, f, indent=2)\n",
    "\n",
    "print(f\" Output saved to: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
